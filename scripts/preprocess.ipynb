{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate a config file with paths to all the raw data. Each entry of the resulting yaml file specifies the source satellite image and the shapefiles over which to create masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/datadrive/glaciers/\")\n",
    "glaciers_file = data_dir / \"vector_data/Glacier_2005.shp\"\n",
    "clean_g_file = data_dir / \"vector_data/clean.shp\"\n",
    "debris_g_file =  data_dir / \"vector_data/debris.shp\"\n",
    "border_file = data_dir / \"vector_data/hkh.shp\"\n",
    "input_folder = data_dir / \"unique_tiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET = 3 # set to False to run on full data\n",
    "\n",
    "paths = {}\n",
    "input_paths = list(Path(input_folder).iterdir())\n",
    "if SUBSET is not False:\n",
    "    input_paths = input_paths[:SUBSET]\n",
    "\n",
    "for i, f in enumerate(input_paths):\n",
    "    mask_ele = {}\n",
    "    mask_ele[\"img_path\"] = str(f)\n",
    "    mask_ele[\"mask_paths\"] = [str(s) for s in [glaciers_file, clean_g_file, debris_g_file]]\n",
    "    mask_ele[\"border_path\"] = str(border_file )\n",
    "    paths[f\"mask_{i}\"] = mask_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "conf_file = data_dir / \"conf/masking_paths.yaml\"\n",
    "with open(conf_file, 'w') as f:\n",
    "    yaml.dump(paths, f, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the configuration file `masking_paths.yaml`, we can create numpy masks that are aligned with the underlying numpy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_mapping.data.mask import generate_masks\n",
    "import shutil\n",
    "\n",
    "masking_paths = yaml.safe_load(open(conf_file))\n",
    "img_paths = [p[\"img_path\"] for p in masking_paths.values()]\n",
    "mask_paths = [p[\"mask_paths\"] for p in masking_paths.values()]\n",
    "border_paths = [p[\"border_path\"] for p in masking_paths.values()]\n",
    "out_dir = Path(data_dir / \"processed_exper/masks\")\n",
    "\n",
    "if out_dir.exists():\n",
    "    shutil.rmtree(out_dir)\n",
    "    \n",
    "generate_masks(img_paths, mask_paths, border_paths=border_paths, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "mask_dir = data_dir/ \"processed_exper/masks\"\n",
    "mask = np.load(mask_dir / \"mask_01.npy\")\n",
    "border = np.load(mask_dir / \"border_01.npy\")\n",
    "_, ax = plt.subplots(1, 4, figsize=(15, 15))\n",
    "ax[0].imshow(mask[:, :, 0])\n",
    "ax[1].imshow(mask[:, :, 1])\n",
    "ax[2].imshow(mask[:, :, 2])\n",
    "ax[3].imshow(border)\n",
    "\n",
    "mask_df = pd.read_csv(mask_dir / \"mask_metadata.csv\")\n",
    "mask_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have binary masks associated with each image, we can slice them into 512 x 512 patches to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_mapping.data.slice import write_pair_slices\n",
    "\n",
    "processed_dir = data_dir / \"processed_exper\"\n",
    "paths = pd.read_csv(processed_dir / \"masks\" / \"mask_metadata.csv\")\n",
    "output_dir = processed_dir / \"patches\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata = []\n",
    "for row in range(len(paths)):\n",
    "    print(f\"## Slicing tiff {row +1}/{len(paths)} ...\")\n",
    "    metadata_ = write_pair_slices(\n",
    "        paths.iloc[row][\"img\"],\n",
    "        paths.iloc[row][\"mask\"],\n",
    "        output_dir,\n",
    "        border_path=paths.iloc[row][\"border\"],\n",
    "        out_base=f\"patch_{paths.index[row]}\"\n",
    "    )\n",
    "    metadata.append(metadata_)\n",
    "\n",
    "metadata = pd.concat(metadata, axis=0)\n",
    "out_path = Path(output_dir, \"patches.geojson\")\n",
    "metadata.to_file(out_path, index=False, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_mapping.data.slice import plot_slices\n",
    "plot_slices(processed_dir / \"patches\", n_cols=4, div=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have sliced each tiff into small patches, we can determine which to use for training, validation, and testing. We first filter away those patches that have relatively little glacier, then we randomly shuffle them into train, dev, and test directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "import geopandas as gpd\n",
    "import glacier_mapping.data.process_slices_funs as pf\n",
    "import yaml\n",
    "\n",
    "processed_dir = data_dir / \"processed_exper\"\n",
    "output_dir = processed_dir / \"patches\"\n",
    "\n",
    "pconf = Dict(yaml.safe_load(open(data_dir / \"conf/postprocess.yaml\", \"r\")))\n",
    "slice_meta = gpd.read_file(output_dir / \"patches.geojson\")\n",
    "\n",
    "# filter all the slices to the ones that matter\n",
    "print(\"filtering\")\n",
    "keep_ids = pf.filter_directory(\n",
    "    slice_meta,\n",
    "    filter_perc=pconf.filter_percentage,\n",
    "    filter_channel=pconf.filter_channel,\n",
    ")\n",
    "\n",
    "# validation: get ids for the ones that will be training vs. testing.\n",
    "print(\"reshuffling\")\n",
    "split_method = [item for item in pconf.split_method.items()][0][0]\n",
    "split_ratio = pconf.split_method[split_method].split_ratio\n",
    "split_fun = getattr(pf, split_method)\n",
    "split_ids = split_fun(keep_ids, split_ratio, slice_meta=slice_meta)\n",
    "target_locs = pf.reshuffle(split_ids, processed_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference, it's useful to save which patches went into which split. These will be contained in the `target_locks.pickle` file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save target_locs\n",
    "import pickle\n",
    "target_locs_file = processed_dir / \"target_locs.pickle\"\n",
    "with open(target_locs_file, \"wb\") as f:\n",
    "    pickle.dump(target_locs, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glaciers",
   "language": "python",
   "name": "glaciers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
