{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "target_locs_file = \"/datadrive/glaciers/processed_exper/target_locs.pickle\"\n",
    "with open(target_locs_file, \"rb\") as file:\n",
    "    target_locs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exper no i\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "from addict import Dict\n",
    "\n",
    "import glacier_mapping.data.process_slices_funs as pf\n",
    "\n",
    "print(\"getting stats\")\n",
    "\n",
    "pconf = Dict(yaml.safe_load(open(f\"/datadrive/glaciers/conf/channel_exp/postprocess_{i}.yaml\", \"r\")))\n",
    "processed_dir = pathlib.Path(\"/datadrive/glaciers\", \"processed_exper\")\n",
    "pconf.process_funs.normalize.stats_path = processed_dir / \\\n",
    "    pathlib.Path(pconf.process_funs.normalize.stats_path)\n",
    "\n",
    "stats = pf.generate_stats(\n",
    "    [p[\"img\"] for p in target_locs[\"train\"]],\n",
    "    pconf.normalization_sample_size,\n",
    "    pconf.process_funs.normalize.stats_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase there is an old folder\n",
    "# !rm -r /datadrive/glaciers/processed_exper/train_1\n",
    "# !rm -r /datadrive/glaciers/processed_exper/val_1\n",
    "# !rm -r /datadrive/glaciers/processed_exper/test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "for split_type in target_locs:\n",
    "    path = f\"/datadrive/glaciers/processed_exper/{split_type}\".replace(split_type, f\"{split_type}_{i}\")\n",
    "    os.mkdir(path)  \n",
    "\n",
    "# postprocess individual images (all the splits)\n",
    "for split_type in target_locs:\n",
    "    print(f\"postprocessing {split_type}...\")\n",
    "    for k in range(len(target_locs[split_type])):\n",
    "        img, mask = pf.postprocess(\n",
    "            target_locs[split_type][k][\"img\"],\n",
    "            target_locs[split_type][k][\"mask\"],\n",
    "            pconf.process_funs,\n",
    "        )\n",
    "        \n",
    "        img_loc = pathlib.Path(str(target_locs[split_type][k][\"img\"]).replace(split_type, f\"{split_type}_{i}\"))\n",
    "        mask_loc = pathlib.Path(str(target_locs[split_type][k][\"mask\"]).replace(split_type, f\"{split_type}_{i}\"))\n",
    "        np.save(img_loc, img)\n",
    "        np.save(mask_loc, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r /datadrive/glaciers/run_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_mapping.data.data import fetch_loaders\n",
    "from glacier_mapping.models.frame import Framework\n",
    "import glacier_mapping.train as tr\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from glacier_mapping.models.metrics import diceloss\n",
    "import torch\n",
    "import json\n",
    "\n",
    "data_dir = pathlib.Path(\"/datadrive/glaciers\")\n",
    "conf = Dict(yaml.safe_load(open(f\"/datadrive/glaciers/conf/channel_exp/train_{i}.yaml\", \"r\")))\n",
    "processed_dir = data_dir / \"processed_exper\"\n",
    "\n",
    "args = Dict({\n",
    "    \"batch_size\": 16,\n",
    "    \"run_name\": f\"run_{i}\",\n",
    "    \"epochs\": 100,\n",
    "    \"save_every\": 20\n",
    "})\n",
    "\n",
    "loaders = fetch_loaders(processed_dir, args.batch_size,\n",
    "                        train_folder=processed_dir/f\"train_{i}\", dev_folder=processed_dir/f\"dev_{i}\")\n",
    "device = torch.device('cuda:0')\n",
    "frame = Framework(\n",
    "    model_opts=conf.model_opts,\n",
    "    optimizer_opts=conf.optim_opts,\n",
    "    reg_opts=conf.reg_opts,\n",
    "    device=device,\n",
    "    loss_fn=diceloss(act=torch.nn.Softmax(dim=1), w=[1, 0])\n",
    ")\n",
    "# for multi-class change diceloss to be diceloss(act=torch.nn.Softmax(dim=1), w=[1, 1, 0])\n",
    "\n",
    "# Setup logging\n",
    "writer = SummaryWriter(f\"{data_dir}/runs/{args.run_name}/logs/\")\n",
    "writer.add_text(\"Arguments\", json.dumps(vars(args)))\n",
    "writer.add_text(\"Configuration Parameters\", json.dumps(conf))\n",
    "out_dir = f\"{data_dir}/runs/{args.run_name}/models/\"\n",
    "\n",
    "best_epoch, best_iou = None, 0\n",
    "for epoch in range(args.epochs):\n",
    "    mask_names = conf.log_opts.mask_names\n",
    "    # train loop\n",
    "    loss_d = {}\n",
    "    loss_d[\"train\"], metrics = tr.train_epoch(loaders[\"train\"], frame, conf.metrics_opts)\n",
    "    tr.log_metrics(writer, metrics, loss_d[\"train\"], epoch, mask_names=mask_names)\n",
    "    tr.log_images(writer, frame, next(iter(loaders[\"train\"])), epoch)\n",
    "\n",
    "    # validation loop\n",
    "    loss_d[\"val\"], metrics = tr.validate(loaders[\"val\"], frame, conf.metrics_opts)\n",
    "    tr.log_metrics(writer, metrics, loss_d[\"val\"], epoch, \"val\", mask_names=mask_names)\n",
    "    tr.log_images(writer, frame, next(iter(loaders[\"val\"])), epoch, \"val\")\n",
    "\n",
    "    # Save model\n",
    "    writer.add_scalars(\"Loss\", loss_d, epoch)\n",
    "    if epoch % args.save_every == 0:\n",
    "        frame.save(out_dir, epoch)\n",
    "    # Save best model according to given metric\n",
    "#     if best_iou <= metrics['IoU'][0]:\n",
    "#         best_iou  = metrics['IoU'][0]\n",
    "#         best_epoch = epoch\n",
    "#         frame.save(out_dir, f\"best model_epoch_{epoch}\")\n",
    "\n",
    "    print(f\"{epoch}/{args.epochs} | train: {loss_d['train']} | val: {loss_d['val']}\")\n",
    "\n",
    "frame.save(out_dir, \"final\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier",
   "language": "python",
   "name": "glacier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
