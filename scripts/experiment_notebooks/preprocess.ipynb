{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all Paths to a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate conf file for all slices\n",
    "glaciers_file = \"/datadrive/glaciers/vector_data/Glacier_2005.shp\"\n",
    "clean_g_file = \"/datadrive/glaciers/vector_data/clean.shp\"\n",
    "debris_g_file = \"/datadrive/glaciers/vector_data/debris.shp\"\n",
    "border_file = \"/datadrive/glaciers/vector_data/hkh.shp\"\n",
    "\n",
    "input_folder = \"/datadrive/glaciers/unique_tiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "paths = {}\n",
    "for i, f in enumerate(pathlib.Path(input_folder).iterdir()):\n",
    "    mask_ele = {}\n",
    "    mask_ele[\"img_path\"] = str(f)\n",
    "    mask_ele[\"mask_paths\"] = [glaciers_file, clean_g_file, debris_g_file]\n",
    "    mask_ele[\"border_path\"] = border_file \n",
    "    paths[f\"mask_{i}\"] = mask_ele \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "conf_file = \"/datadrive/glaciers/conf/masking_paths_all_data.yaml\"\n",
    "with open(conf_file, 'w') as file:\n",
    "    yaml.dump(paths, file, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase there is an old folder\n",
    "# !rm -rf /datadrive/glaciers/processed_exper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maksing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from glacier_mapping.data.mask import generate_masks\n",
    "\n",
    "masking_paths = yaml.load(open(conf_file))\n",
    "img_paths = [p[\"img_path\"] for p in masking_paths.values()]\n",
    "mask_paths = [p[\"mask_paths\"] for p in masking_paths.values()]\n",
    "border_paths = [p[\"border_path\"] for p in masking_paths.values()]\n",
    "out_dir = pathlib.Path(\"/datadrive/glaciers/processed_exper/masks\")\n",
    "generate_masks(img_paths, mask_paths, border_paths=border_paths, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "mask_dir = pathlib.Path(\"/datadrive/glaciers/\", \"processed_exper\", \"masks\")\n",
    "mask = np.load(mask_dir / \"mask_05.npy\")\n",
    "border = np.load(mask_dir / \"border_05.npy\")\n",
    "print(mask.shape)\n",
    "_, ax = plt.subplots(1, 4, figsize=(15, 15))\n",
    "ax[0].imshow(mask[:, :, 0])\n",
    "ax[1].imshow(mask[:, :, 1])\n",
    "ax[2].imshow(mask[:, :, 2])\n",
    "ax[3].imshow(border)\n",
    "\n",
    "mask_df = pd.read_csv(mask_dir / \"mask_metadata.csv\")\n",
    "mask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase there is an old folder\n",
    "# !rm -r /datadrive/glaciers/processed_exper/slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "from glacier_mapping.data.slice import write_pair_slices\n",
    "\n",
    "processed_dir = pathlib.Path(\"/datadrive/glaciers\", \"processed_exper\")\n",
    "paths = pd.read_csv(processed_dir / \"masks\" / \"mask_metadata.csv\")\n",
    "output_dir = processed_dir / \"slices\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata = []\n",
    "for row in range(len(paths)):\n",
    "    print(f\"## Slicing tiff {row +1}/{len(paths)} ...\")\n",
    "    metadata_ = write_pair_slices(\n",
    "        paths.iloc[row][\"img\"],\n",
    "        paths.iloc[row][\"mask\"],\n",
    "        output_dir,\n",
    "        border_path=paths.iloc[row][\"border\"],\n",
    "        out_base=f\"slice_{paths.index[row]}\"\n",
    "    )\n",
    "    metadata.append(metadata_)\n",
    "\n",
    "metadata = pd.concat(metadata, axis=0)\n",
    "out_path = pathlib.Path(output_dir, \"slices.geojson\")\n",
    "metadata.to_file(out_path, index=False, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_mapping.data.slice import plot_slices\n",
    "plot_slices(\"/datadrive/glaciers/processed_exper/slices/\", n_cols=4, div=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import glacier_mapping.data.process_slices_funs as pf\n",
    "\n",
    "output_dir = pathlib.Path(\"/datadrive/glaciers/\", \"processed_exper\")\n",
    "pconf = Dict(yaml.safe_load(open(\"/datadrive/glaciers/conf/postprocess.yaml\", \"r\")))\n",
    "slice_meta = gpd.read_file(pathlib.Path(output_dir, \"slices\", \"slices.geojson\"))\n",
    "\n",
    "# filter all the slices to the ones that matter\n",
    "print(\"filtering\")\n",
    "keep_ids = pf.filter_directory(\n",
    "    slice_meta,\n",
    "    filter_perc=pconf.filter_percentage,\n",
    "    filter_channel=pconf.filter_channel,\n",
    ")\n",
    "\n",
    "# validation: get ids for the ones that will be training vs. testing.\n",
    "print(\"reshuffling\")\n",
    "split_method = [item for item in pconf.split_method.items()][0][0]\n",
    "split_ratio = pconf.split_method[split_method].split_ratio\n",
    "split_fun = getattr(pf, split_method)\n",
    "split_ids = split_fun(keep_ids, split_ratio, slice_meta=slice_meta)\n",
    "target_locs = pf.reshuffle(split_ids, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import glacier_mapping.data.process_slices_funs as pf\n",
    "\n",
    "output_dir = pathlib.Path(\"/datadrive/glaciers/\", \"processed_exper\")\n",
    "pconf = Dict(yaml.safe_load(open(\"/datadrive/glaciers/conf/postprocess.yaml\", \"r\")))\n",
    "slice_meta = gpd.read_file(pathlib.Path(output_dir, \"slices\", \"slices.geojson\"))\n",
    "\n",
    "# filter all the slices to the ones that matter\n",
    "print(\"filtering\")\n",
    "keep_ids = pf.filter_directory(\n",
    "    slice_meta,\n",
    "    filter_perc=pconf.filter_percentage,\n",
    "    filter_channel=pconf.filter_channel,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save target_locs\n",
    "import pickle\n",
    "target_locs_file = '/datadrive/glaciers/processed_exper/target_locs.pickle'\n",
    "with open(target_locs_file, 'wb') as f:\n",
    "    pickle.dump(target_locs, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier",
   "language": "python",
   "name": "glacier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
