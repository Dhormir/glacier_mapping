{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform training. We use the train, dev, and test folders specified above, and our logging is done in tensorboard. The model is saved every `save_every` epochs; the model with the best IoU is also saved under the name `model_best`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"glacier_data\")\n",
    "process_dir = data_dir / \"splits\"\n",
    "\n",
    "args = Dict({\n",
    "    \"batch_size\": 16,\n",
    "    \"run_name\": \"demo\", \n",
    "    \"epochs\": 200,\n",
    "    \"save_every\": 50,\n",
    "    \"loss_type\": \"dice\",\n",
    "    \"device\": \"cuda:0\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repositories\\glacier_mapping\\glacier_mapping\\models\\frame.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.long, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/200 | train: 0.054354348008377434 | val: 0.06277614831924438\n",
      "1/200 | train: 0.051559342105457116 | val: 0.05159737305207686\n",
      "2/200 | train: 0.04373037737281142 | val: 0.05053310069170865\n",
      "3/200 | train: 0.04218079838366795 | val: 0.046574049646204166\n",
      "4/200 | train: 0.04125892524619638 | val: 0.046028152379122646\n",
      "5/200 | train: 0.04040143975389844 | val: 0.04516075090928511\n",
      "6/200 | train: 0.03955614099925865 | val: 0.04464033408598466\n",
      "7/200 | train: 0.038797521248809974 | val: 0.043514727462421764\n",
      "8/200 | train: 0.03800808854264939 | val: 0.04281104369597002\n",
      "9/200 | train: 0.03688909101735207 | val: 0.04028119065544822\n",
      "10/200 | train: 0.03624522094626962 | val: 0.04007151343605735\n",
      "11/200 | train: 0.035609041274994535 | val: 0.03901559222828258\n",
      "12/200 | train: 0.03527434360887612 | val: 0.03795452659780329\n",
      "13/200 | train: 0.03484513118435135 | val: 0.03853121128949252\n",
      "14/200 | train: 0.0342893503197802 | val: 0.03747232773087241\n",
      "15/200 | train: 0.03408183278053921 | val: 0.03803189613602378\n",
      "16/200 | train: 0.03367090894410255 | val: 0.03766183094544844\n",
      "17/200 | train: 0.03386289488242127 | val: 0.037822935797951436\n",
      "18/200 | train: 0.033471758231793 | val: 0.03769858967174183\n",
      "19/200 | train: 0.03303431359657108 | val: 0.035165022178129716\n",
      "20/200 | train: 0.03302331949649841 | val: 0.036872148513793945\n",
      "21/200 | train: 0.03245302388935736 | val: 0.036516209082169966\n",
      "22/200 | train: 0.03299520913676867 | val: 0.036320413784547284\n",
      "23/200 | train: 0.032177870096176786 | val: 0.03405626307834279\n",
      "24/200 | train: 0.031913606367285505 | val: 0.035271312431855634\n",
      "25/200 | train: 0.03073324928395742 | val: 0.03338417952710932\n",
      "26/200 | train: 0.031050212934185258 | val: 0.03382638205181469\n",
      "27/200 | train: 0.030781627323857797 | val: 0.033754439787431194\n",
      "28/200 | train: 0.029991545347258567 | val: 0.037342547286640514\n",
      "29/200 | train: 0.03000753752245293 | val: 0.0332673885605552\n",
      "30/200 | train: 0.029847028983790942 | val: 0.03224250294945457\n",
      "31/200 | train: 0.02934061422671724 | val: 0.03219021721319719\n",
      "32/200 | train: 0.029069706603067662 | val: 0.03262674104083668\n",
      "33/200 | train: 0.029113077697181203 | val: 0.03613772717389194\n",
      "34/200 | train: 0.029236087988935625 | val: 0.03447147391059182\n",
      "35/200 | train: 0.028776820894943517 | val: 0.0321537429636175\n",
      "36/200 | train: 0.029222487780817496 | val: 0.0319777862592177\n",
      "37/200 | train: 0.02860430163129503 | val: 0.03045017123222351\n",
      "38/200 | train: 0.02857941381302577 | val: 0.032528509335084395\n",
      "39/200 | train: 0.027584382821623402 | val: 0.03081441955132918\n",
      "40/200 | train: 0.027891232902013913 | val: 0.030584501136432993\n",
      "41/200 | train: 0.027335150201389123 | val: 0.029094859144904398\n",
      "42/200 | train: 0.02756050952732096 | val: 0.030669259483164006\n",
      "43/200 | train: 0.02802749993595691 | val: 0.029436511885036123\n",
      "44/200 | train: 0.026991777491631768 | val: 0.03134353215044195\n",
      "45/200 | train: 0.027847486360888567 | val: 0.032079740004106\n",
      "46/200 | train: 0.027853405895805858 | val: 0.031882580301978374\n",
      "47/200 | train: 0.027050303339024438 | val: 0.030213642662221735\n",
      "48/200 | train: 0.027695348384174916 | val: 0.030862881378694015\n",
      "49/200 | train: 0.027219248969623376 | val: 0.029581064527684994\n",
      "50/200 | train: 0.027146809406131116 | val: 0.031006299365650525\n",
      "51/200 | train: 0.026826174545537085 | val: 0.02911923419345509\n",
      "52/200 | train: 0.026460592491508464 | val: 0.030475478822534735\n",
      "53/200 | train: 0.02710614581020943 | val: 0.028885357488285412\n",
      "54/200 | train: 0.02606779399179603 | val: 0.030400527065450496\n",
      "55/200 | train: 0.02637347392251535 | val: 0.02993441278284246\n",
      "56/200 | train: 0.026224513511433612 | val: 0.029683626239950005\n",
      "57/200 | train: 0.026310530950755424 | val: 0.027668024193156848\n",
      "58/200 | train: 0.026143277127188738 | val: 0.027763913436369463\n",
      "59/200 | train: 0.026576331327851695 | val: 0.028359172560951926\n",
      "60/200 | train: 0.025960207141099336 | val: 0.029683211716738614\n",
      "61/200 | train: 0.026355523120018584 | val: 0.028168692913922398\n",
      "62/200 | train: 0.026491951148752756 | val: 0.027810662984848022\n",
      "63/200 | train: 0.02523414109767884 | val: 0.02911493398926475\n",
      "64/200 | train: 0.02591775914085129 | val: 0.027707117254083806\n",
      "65/200 | train: 0.025367886761772415 | val: 0.02727531303058971\n",
      "66/200 | train: 0.025676202836298134 | val: 0.028549522161483765\n",
      "67/200 | train: 0.02551184295051714 | val: 0.03043791868469932\n",
      "68/200 | train: 0.025800395727780097 | val: 0.02808179801160639\n",
      "69/200 | train: 0.02512641317228113 | val: 0.027390981262380428\n",
      "70/200 | train: 0.025402199973014253 | val: 0.028105139190500433\n",
      "71/200 | train: 0.02501233470657787 | val: 0.02879751758141951\n",
      "72/200 | train: 0.025169556041299206 | val: 0.03153148510239341\n",
      "73/200 | train: 0.025531089025751418 | val: 0.02823705944147977\n",
      "74/200 | train: 0.02489395303452295 | val: 0.02894505099816756\n",
      "75/200 | train: 0.024931039028939627 | val: 0.028281048211184413\n",
      "76/200 | train: 0.02476487442967786 | val: 0.028904055465351453\n",
      "77/200 | train: 0.02569411862301142 | val: 0.02852845300327648\n",
      "78/200 | train: 0.02487506303090028 | val: 0.030306779796426948\n",
      "79/200 | train: 0.024912935090749756 | val: 0.02675668326291171\n",
      "80/200 | train: 0.024845967171397594 | val: 0.026549791206013074\n",
      "81/200 | train: 0.02494154594274476 | val: 0.0281476844440807\n",
      "82/200 | train: 0.02551446166424465 | val: 0.027418616685000334\n",
      "83/200 | train: 0.025212414893406802 | val: 0.027565032243728638\n",
      "84/200 | train: 0.024783423535195095 | val: 0.02980997454036366\n",
      "85/200 | train: 0.024956708010431993 | val: 0.0273880042813041\n",
      "86/200 | train: 0.024841284471765823 | val: 0.026570993661880492\n",
      "87/200 | train: 0.02437924591741736 | val: 0.02763123620640148\n",
      "88/200 | train: 0.024888597564971165 | val: 0.0280218628319827\n",
      "89/200 | train: 0.024528077003538765 | val: 0.027261746471578426\n",
      "90/200 | train: 0.024452685376060226 | val: 0.02804702195254239\n",
      "91/200 | train: 0.02428608642545755 | val: 0.02739216847853227\n",
      "92/200 | train: 0.024367616475406577 | val: 0.026754791086370296\n",
      "93/200 | train: 0.024595614435156083 | val: 0.026340565356341276\n",
      "94/200 | train: 0.02403915768815083 | val: 0.027716211839155718\n",
      "95/200 | train: 0.024575355859089147 | val: 0.02718428752639077\n",
      "96/200 | train: 0.024131791165229857 | val: 0.026836550777608697\n",
      "97/200 | train: 0.023796689416970038 | val: 0.027592058073390612\n",
      "98/200 | train: 0.024555122027509205 | val: 0.026942057501186025\n",
      "99/200 | train: 0.023710509115348596 | val: 0.026731684533032506\n",
      "100/200 | train: 0.02363645804768754 | val: 0.027145783467726274\n",
      "101/200 | train: 0.023955248899310437 | val: 0.027102410793304443\n",
      "102/200 | train: 0.023341883489419523 | val: 0.02711193344809792\n",
      "103/200 | train: 0.023902801480679227 | val: 0.027489453012293035\n",
      "104/200 | train: 0.024237478604204662 | val: 0.02673114158890464\n",
      "105/200 | train: 0.02364140570008101 | val: 0.02596789977767251\n",
      "106/200 | train: 0.023232486633968104 | val: 0.026751480319283226\n",
      "107/200 | train: 0.023091197091667834 | val: 0.026753642884167757\n",
      "108/200 | train: 0.02401731149646072 | val: 0.02538404085419395\n",
      "109/200 | train: 0.023474118728239294 | val: 0.02734422521157698\n",
      "110/200 | train: 0.023435644785978153 | val: 0.025820123607462102\n",
      "111/200 | train: 0.02373670605704305 | val: 0.026235026662999934\n",
      "112/200 | train: 0.023946601245795465 | val: 0.026924903826280072\n",
      "113/200 | train: 0.023422087686180135 | val: 0.02656697089021856\n",
      "114/200 | train: 0.02371223640504145 | val: 0.02588366757739674\n",
      "115/200 | train: 0.023277924904312852 | val: 0.025993632728403264\n",
      "116/200 | train: 0.023605139464062126 | val: 0.025608075206929988\n",
      "117/200 | train: 0.023323930975040007 | val: 0.027402128414674237\n",
      "118/200 | train: 0.023006192308804387 | val: 0.025073645331642844\n",
      "119/200 | train: 0.023151372571526867 | val: 0.025788696787574075\n",
      "120/200 | train: 0.02389689377331547 | val: 0.025686901265924628\n",
      "121/200 | train: 0.022877480124680244 | val: 0.02592994679104198\n",
      "122/200 | train: 0.023193940092315873 | val: 0.02456427812576294\n",
      "123/200 | train: 0.023055545265929817 | val: 0.024078184908086605\n",
      "124/200 | train: 0.022568795839737973 | val: 0.02567082643508911\n",
      "125/200 | train: 0.023079441048146538 | val: 0.026394578543576328\n",
      "126/200 | train: 0.0234201351282802 | val: 0.025442538478157736\n",
      "127/200 | train: 0.02258825574469006 | val: 0.02661305395039645\n",
      "128/200 | train: 0.02275860986572644 | val: 0.024997108632867988\n",
      "129/200 | train: 0.023268273465004666 | val: 0.025489497726613825\n",
      "130/200 | train: 0.023229087612958863 | val: 0.024687755649740045\n",
      "131/200 | train: 0.022317670110622835 | val: 0.02426294142549688\n",
      "132/200 | train: 0.022250541585543758 | val: 0.024308168346231635\n",
      "133/200 | train: 0.02266934109418884 | val: 0.025345090844414452\n",
      "134/200 | train: 0.022771484780249335 | val: 0.02440089366652749\n",
      "135/200 | train: 0.02259237188894506 | val: 0.024194895137440074\n",
      "136/200 | train: 0.022126845140058752 | val: 0.025952788374640726\n",
      "137/200 | train: 0.02250525059650212 | val: 0.02465539682995189\n",
      "138/200 | train: 0.02227945435140525 | val: 0.027087887850674716\n",
      "139/200 | train: 0.022587391477340818 | val: 0.02399438294497403\n",
      "140/200 | train: 0.022105862671027918 | val: 0.023808323253284802\n",
      "141/200 | train: 0.022595764762738977 | val: 0.02468077161095359\n",
      "142/200 | train: 0.021891780528325015 | val: 0.024923598766326903\n",
      "143/200 | train: 0.02171292737632134 | val: 0.024117088317871092\n",
      "144/200 | train: 0.021931037656943417 | val: 0.02378755049272017\n",
      "145/200 | train: 0.02180164149784855 | val: 0.023579282652248037\n",
      "146/200 | train: 0.021655635718577215 | val: 0.023458033800125122\n",
      "147/200 | train: 0.021361829484411694 | val: 0.024403586170890116\n",
      "148/200 | train: 0.021494373095564682 | val: 0.024491982026533646\n",
      "149/200 | train: 0.021528295530662836 | val: 0.02370255481113087\n",
      "150/200 | train: 0.02113649093451139 | val: 0.02428828477859497\n",
      "151/200 | train: 0.02170679541231758 | val: 0.023801814967935736\n",
      "152/200 | train: 0.021400153247868113 | val: 0.02281619202006947\n",
      "153/200 | train: 0.02152090758941192 | val: 0.023737773028287022\n",
      "154/200 | train: 0.021060594145996453 | val: 0.02377511913126165\n",
      "155/200 | train: 0.02086907430666234 | val: 0.022569173032587226\n",
      "156/200 | train: 0.02084611507993454 | val: 0.023874487660147926\n",
      "157/200 | train: 0.02085977255831188 | val: 0.02234047055244446\n",
      "158/200 | train: 0.02048640400560344 | val: 0.023076502843336625\n",
      "159/200 | train: 0.0203998572054482 | val: 0.02169005112214522\n",
      "160/200 | train: 0.020766339476363777 | val: 0.022629890116778288\n",
      "161/200 | train: 0.020335080340387928 | val: 0.021455648812380705\n",
      "162/200 | train: 0.01997197281287171 | val: 0.022167622501199895\n",
      "163/200 | train: 0.020165578892585814 | val: 0.020704608071934093\n",
      "164/200 | train: 0.019798576754004774 | val: 0.02197193611751903\n",
      "165/200 | train: 0.02002070957002067 | val: 0.020725382458079946\n",
      "166/200 | train: 0.02017587462231011 | val: 0.02228386727246371\n",
      "167/200 | train: 0.01979708943914807 | val: 0.022195588458668102\n",
      "168/200 | train: 0.02004181568367363 | val: 0.021498138254339046\n",
      "169/200 | train: 0.019936011214169137 | val: 0.020336420969529586\n",
      "170/200 | train: 0.019494633372707716 | val: 0.020998124100945214\n",
      "171/200 | train: 0.019147989255018708 | val: 0.019981092485514555\n",
      "172/200 | train: 0.019133016350375145 | val: 0.021178339286284015\n",
      "173/200 | train: 0.018931047956875037 | val: 0.020463030988519842\n",
      "174/200 | train: 0.018695147284950975 | val: 0.02075754620812156\n",
      "175/200 | train: 0.01939251536488844 | val: 0.02045242190361023\n",
      "176/200 | train: 0.01873324519660392 | val: 0.02140885428948836\n",
      "177/200 | train: 0.019069546040915944 | val: 0.02051687484437769\n",
      "178/200 | train: 0.019017364820052066 | val: 0.019730080528692767\n",
      "179/200 | train: 0.018650296462733813 | val: 0.01965416046706113\n",
      "180/200 | train: 0.01868542032826974 | val: 0.01926911554553292\n",
      "181/200 | train: 0.018247681015776594 | val: 0.020517089692029086\n",
      "182/200 | train: 0.01815909100574862 | val: 0.019763911041346463\n",
      "183/200 | train: 0.018673943944139206 | val: 0.020335760712623597\n",
      "184/200 | train: 0.018080517610744147 | val: 0.01930465264753862\n",
      "185/200 | train: 0.0180544845429164 | val: 0.01950304237279025\n",
      "186/200 | train: 0.017918457483809547 | val: 0.019575163451108067\n",
      "187/200 | train: 0.01769145471153309 | val: 0.01989489035172896\n",
      "188/200 | train: 0.01743790133814899 | val: 0.019648771394382824\n",
      "189/200 | train: 0.018108941867519608 | val: 0.02029472589492798\n",
      "190/200 | train: 0.017586523924732957 | val: 0.019062576510689475\n",
      "191/200 | train: 0.017959340589787258 | val: 0.01942176493731412\n",
      "192/200 | train: 0.017502009090493303 | val: 0.0186874506148425\n",
      "193/200 | train: 0.01740911980366271 | val: 0.019261778484691273\n",
      "194/200 | train: 0.01731895544510286 | val: 0.01854466145688837\n",
      "195/200 | train: 0.016965503870040255 | val: 0.018893044645136053\n",
      "196/200 | train: 0.01667491757060467 | val: 0.017654714530164546\n",
      "197/200 | train: 0.017058650133815197 | val: 0.018357308886267923\n",
      "198/200 | train: 0.01677769492562695 | val: 0.017905889586968857\n",
      "199/200 | train: 0.016908501676106268 | val: 0.017147832296111366\n"
     ]
    }
   ],
   "source": [
    "from glacier_mapping.data.data import fetch_loaders\n",
    "from glacier_mapping.models.frame import Framework\n",
    "import glacier_mapping.train as tr\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from glacier_mapping.models.metrics import diceloss\n",
    "import yaml\n",
    "import torch\n",
    "import json\n",
    "\n",
    "conf = Dict(yaml.safe_load(open(\"conf/train.yaml\", \"r\")))\n",
    "loaders = fetch_loaders(process_dir, args.batch_size)\n",
    "device = torch.device(args.device)\n",
    "\n",
    "loss_fn = None\n",
    "outchannels = conf.model_opts.args.outchannels\n",
    "if args.loss_type == \"dice\":\n",
    "    loss_fn = diceloss(\n",
    "        act=torch.nn.Softmax(dim=1), \n",
    "        w=[0.6, 0.9, 0.2], # clean ice, debris, background\n",
    "        outchannels=outchannels, \n",
    "        label_smoothing=0.2\n",
    "    )\n",
    "    \n",
    "frame = Framework(\n",
    "    model_opts=conf.model_opts,\n",
    "    optimizer_opts=conf.optim_opts,\n",
    "    reg_opts=conf.reg_opts,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "writer = SummaryWriter(f\"{data_dir}/{args.run_name}/logs/\")\n",
    "writer.add_text(\"Arguments\", json.dumps(vars(args)))\n",
    "writer.add_text(\"Configuration Parameters\", json.dumps(conf))\n",
    "out_dir = f\"{data_dir}/{args.run_name}/models/\"\n",
    "\n",
    "best_epoch, best_iou = None, 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_d = {}\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        loss_d[phase], metrics = tr.train_epoch(loaders[phase], frame, conf.metrics_opts)\n",
    "        tr.log_metrics(writer, metrics, loss_d[phase], epoch, phase, mask_names=conf.log_opts.mask_names)\n",
    "\n",
    "    # save model\n",
    "    writer.add_scalars(\"Loss\", loss_d, epoch)\n",
    "    if (epoch + 1) % args.save_every == 0:\n",
    "        frame.save(out_dir, epoch)\n",
    "        tr.log_images(writer, frame, next(iter(loaders[\"train\"])), epoch)\n",
    "        tr.log_images(writer, frame, next(iter(loaders[\"val\"])), epoch, \"val\")\n",
    "\n",
    "    if best_iou <= metrics['IoU'][0]:\n",
    "        best_iou  = metrics['IoU'][0]\n",
    "        best_epoch = epoch\n",
    "        frame.save(out_dir, \"best\")\n",
    "\n",
    "    print(f\"{epoch}/{args.epochs} | train: {loss_d['train']} | val: {loss_d['val']}\")\n",
    "\n",
    "frame.save(out_dir, \"final\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
